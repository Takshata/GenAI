{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46P_R-qEUNMs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import google.generativeai as genai\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "SjXx5RyiVCDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY='AIzaSyCb7gl4i4JWE55Ib8hYnn915Tp3tWLkK2U'"
      ],
      "metadata": {
        "id": "2PZXv0x4U7GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key='GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "VVBdQ2_dU7Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_API_KEY']='AIzaSyClikSIACiZmLNmauhvfy7_1aIE8UNu04I'"
      ],
      "metadata": {
        "id": "w5QZoVv4r_UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure()"
      ],
      "metadata": {
        "id": "yFUIhCzWsHtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "current_directory=os.getcwd()\n",
        "key_to_file_path=os.path.join(current_directory,'key.json')\n",
        "try:\n",
        "  with open(key_to_file_path) as file:\n",
        "    api_keys=json.load(file)\n",
        "except FileNotFoundError:\n",
        "  print(f'the json.file is not found in the current path{key_to_file_path}')\n",
        "\n",
        "with open('/content/docs.txt','r') as file:\n",
        "  documents=file.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7oiil8wU7LV",
        "outputId": "607e78a0-d263-4784-92a0-16f925f860dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the json.file is not found in the current path/content/key.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "gsFb52ajU7Up",
        "outputId": "23328ed5-f2fa-4487-a503-aa4ba34557c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning algorithms are computational models that allow computers to understand patterns and forecast or make judgments based on data without explicit programming. These algorithms form the foundation of modern artificial intelligence and are used in various applications, including image and speech recognition, natural language processing, recommendation systems, fraud detection, autonomous cars, etc.\\nThis Machine learning Algorithms article will cover all the essential algorithms of machine learning like Support vector machine, decision-making, logistics regression, naive bayees classifier, random forest, k-mean clustering, reinforcement learning, vector, hierarchical clustering, xgboost, adaboost, logistics, etc.\\n\\nTypes of Machine Learning Algorithms\\nThere are four types of machine learning algorithms\\n\\n1. Supervised Learning\\nA. Classification\\nLogistic Regression\\nSupport Vector Machines (SVM)\\nk-Nearest Neighbors (k-NN)\\nNaive Bayes\\nDecision Trees\\nRandom Forest\\nGradient Boosting (e.g., XGBoost, LightGBM, CatBoost)\\nNeural Networks (e.g., Multilayer Perceptron)\\nB. Regression\\nLinear Regression\\nRidge Regression\\nLasso Regression\\nSupport Vector Regression (SVR)\\nDecision Trees Regression\\nRandom Forest Regression\\nGradient Boosting Regression\\nNeural Networks Regression\\n2. Unsupervised Learning\\nA. Clustering\\nk-Means\\nHierarchical Clustering\\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\\nGaussian Mixture Models (GMM)\\nB. Dimensionality Reduction\\nPrincipal Component Analysis (PCA)\\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\\nLinear Discriminant Analysis (LDA)\\nIndependent Component Analysis (ICA)\\nUMAP (Uniform Manifold Approximation and Projection)\\nC. Association\\nApriori Algorithm\\nEclat Algorithm\\n3. Reinforcement Learning\\nA. Model-Free Methods\\nQ-Learning\\nDeep Q-Network (DQN)\\nSARSA (State-Action-Reward-State-Action)\\nPolicy Gradient Methods (e.g., REINFORCE)\\nB. Model-Based Methods\\nDeep Deterministic Policy Gradient (DDPG)\\nProximal Policy Optimization (PPO)\\nTrust Region Policy Optimization (TRPO)\\nC. Value-Based Methods\\nMonte Carlo Methods\\nTemporal Difference (TD) Learning\\n4. Ensemble Learning\\nBagging (e.g., Random Forest)\\nBoosting (e.g., AdaBoost, Gradient Boosting)\\nStacking\\n1. Supervised Learning\\nSupervised learning involves training a model on labeled data, where the desired output is known. The model learns to map inputs to outputs based on the provided examples.\\n\\nA. Classification\\n1. Logistic Regression\\nDescription: Logistic regression models the probability of a binary outcome using a logistic function. It outputs probabilities and classifies instances by setting a threshold (usually 0.5).\\nKey Points:\\nSimple and easy to implement.\\nAssumes linear relationship between the input features and the log-odds of the outcome.\\nWorks well for binary classification problems.\\nApplications: Email spam detection, disease diagnosis, credit scoring.\\n2. Support Vector Machines (SVM)\\nDescription: SVMs find the hyperplane that best separates different classes by maximizing the margin between them.\\nKey Points:\\nEffective in high-dimensional spaces.\\nWorks well for both linear and non-linear classification using kernel trick.\\nSensitive to the choice of kernel and regularization parameter.\\nApplications: Image classification, text categorization, bioinformatics.\\n3. k-Nearest Neighbors (k-NN)\\nDescription: k-NN classifies instances based on the majority class among the k-nearest neighbors in the feature space.\\nKey Points:\\nSimple and intuitive.\\nNo explicit training phase, making it a lazy learner.\\nSensitive to the choice of k and the distance metric.\\nApplications: Recommender systems, pattern recognition, anomaly detection.\\n4. Naive Bayes\\nDescription: Naive Bayes uses Bayesâ€™ theorem with the assumption of feature independence to classify instances.\\nKey Points:\\nFast and efficient.\\nPerforms well with high-dimensional data.\\nAssumption of feature independence might not hold in all cases.\\nApplications: Text classification, sentiment analysis, spam filtering.\\n5. Decision Trees\\nDescription: Decision trees split data into subsets based on the value of input features, creating a tree-like model of decisions.\\nKey Points:\\nEasy to interpret and visualize.\\nCan handle both numerical and categorical data.\\nProne to overfitting without proper pruning.\\nApplications: Risk assessment, fraud detection, customer segmentation.\\n6. Random Forest\\nDescription: Random forest is an ensemble of decision trees that improves accuracy and controls overfitting by averaging multiple trees trained on different subsets of data.\\nKey Points:\\nReduces overfitting compared to individual decision trees.\\nHandles large datasets with higher dimensionality.\\nRequires more computational resources.\\nApplications: Financial forecasting, image classification, healthcare diagnostics.\\n7. Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost)\\nDescription: Gradient boosting builds models sequentially to correct errors made by previous models, optimizing for accuracy.\\nKey Points:\\nHighly accurate and efficient.\\nCan handle different types of data.\\nProne to overfitting if not properly tuned.\\nApplications: Web search ranking, customer churn prediction, insurance risk prediction.\\n8. Neural Networks (e.g., Multilayer Perceptron)\\nDescription: Neural networks use layers of interconnected nodes to model complex patterns in data.\\nKey Points:\\nCapable of learning non-linear relationships.\\nRequires large amounts of data and computational power.\\nCan be prone to overfitting.\\nApplications: Image recognition, speech recognition, natural language processing.\\nB. Regression\\n1. Linear Regression\\nDescription: Linear regression models the relationship between dependent and independent variables using a linear approach.\\nKey Points:\\nSimple and easy to implement.\\nAssumes a linear relationship between the variables.\\nSensitive to outliers.\\nApplications: House price prediction, sales forecasting, risk management.\\n2. Ridge Regression\\nDescription: Ridge regression adds L2 regularization to linear regression to handle multicollinearity and prevent overfitting.\\nKey Points:\\nShrinks coefficients to reduce overfitting.\\nHandles multicollinearity well.\\nRequires tuning of the regularization parameter.\\nApplications: Economic forecasting, portfolio optimization, marketing analysis.\\n3. Lasso Regression\\nDescription: Lasso regression adds L1 regularization to linear regression to perform feature selection by shrinking some coefficients to zero.\\nKey Points:\\nPerforms feature selection.\\nCan produce sparse models.\\nRequires tuning of the regularization parameter.\\nApplications: Gene selection, model selection, finance.\\n4. Support Vector Regression (SVR)\\nDescription: SVR uses support vector machines for regression tasks by finding a function that deviates from the actual target values by a value no greater than a specified margin.\\nKey Points:\\nEffective in high-dimensional spaces.\\nRobust to outliers.\\nSensitive to the choice of kernel and regularization parameter.\\nApplications: Time series prediction, stock price forecasting, real estate valuation.\\n5. Decision Trees Regression\\nDescription: Decision trees regression splits data into subsets to predict continuous values.\\nKey Points:\\nEasy to interpret and visualize.\\nCan handle both numerical and categorical data.\\nProne to overfitting without proper pruning.\\nApplications: Business forecasting, medical diagnosis, engineering.\\n6. Random Forest Regression\\nDescription: Random forest regression is an ensemble of decision trees for regression tasks, averaging the predictions to improve accuracy and control overfitting.\\nKey Points:\\nReduces overfitting compared to individual decision trees.\\nHandles large datasets with higher dimensionality.\\nRequires more computational resources.\\nApplications: Environmental modeling, energy demand forecasting, market analysis.\\n7. Gradient Boosting Regression\\nDescription: Gradient boosting regression sequentially builds models to improve predictions by correcting errors made by previous models.\\nKey Points:\\nHighly accurate and efficient.\\nCan handle different types of data.\\nProne to overfitting if not properly tuned.\\nApplications: Housing price prediction, customer lifetime value prediction, demand forecasting.\\n8. Neural Networks Regression\\nDescription: Neural networks for regression use layers of interconnected nodes to predict continuous values.\\nKey Points:\\nCapable of learning non-linear relationships.\\nRequires large amounts of data and computational power.\\nCan be prone to overfitting.\\nApplications: Energy consumption forecasting, algorithmic trading, weather prediction.\\n2. Unsupervised Learning\\nUnsupervised learning works with unlabeled data and aims to find hidden patterns or intrinsic structures in the input data.\\n\\nA. Clustering\\n1. k-Means\\nDescription: k-Means partitions data into k clusters based on feature similarity, minimizing the sum of squared distances from each point to the centroid of its assigned cluster.\\nKey Points:\\nSimple and efficient.\\nSensitive to the initial placement of centroids.\\nAssumes clusters are spherical.\\nApplications: Customer segmentation, market research, image compression.\\n2. Hierarchical Clustering\\nDescription: Hierarchical clustering builds a hierarchy of clusters using either a bottom-up (agglomerative) or top-down (divisive) approach.\\nKey Points:\\nDoes not require a predefined number of clusters.\\nProduces a dendrogram for visualizing the hierarchy.\\nComputationally intensive for large datasets.\\nApplications: Social network analysis, gene sequence analysis, document clustering.\\n3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\\nDescription: DBSCAN groups together points that are close to each other based on distance and density, and identifies outliers as points that lie alone in low-density regions.\\nKey Points:\\nCan find arbitrarily shaped clusters.\\nRobust to noise and outliers.\\nRequires tuning of the density parameters.\\nApplications: Geographic data analysis, fraud detection, biology.\\n4. Gaussian Mixture Models (GMM)\\nDescription: GMM assumes data is generated from a mixture of several Gaussian distributions, each representing a cluster.\\nKey Points:\\nCan model clusters with different shapes and sizes.\\nUses probabilistic soft assignments of points to clusters.\\nSensitive to initialization and can converge to local optima.\\nApplications: Image segmentation, anomaly detection, finance.\\nB. Dimensionality Reduction\\n1. Principal Component Analysis (PCA)\\nDescription: PCA reduces the dimensionality of data by transforming it to a new set of orthogonal features (principal components) that capture the maximum variance.\\nKey Points:\\nReduces complexity of data.\\nHelps in visualizing high-dimensional data.\\nAssumes linear relationships among features.\\nApplications: Data compression, noise reduction, feature extraction.\\n2. t-Distributed Stochastic Neighbor Embedding (t-SNE)\\nDescription: t-SNE reduces dimensions for visualization by preserving the local structure of the data, making similar points stay close together.\\nKey Points:\\nEffective for visualizing high-dimensional data.\\nComputationally intensive.\\nDoes not preserve global structure well.\\nApplications: Visualizing clusters, exploring high-dimensional data, anomaly detection.\\n3. Linear Discriminant Analysis (LDA)\\nDescription: LDA reduces dimensions by maximizing class separability, transforming data to a space that best discriminates between classes.\\nKey Points:\\nMaximizes class separability.\\nAssumes normally distributed classes with identical covariances.\\nUseful for supervised dimensionality reduction.\\nApplications: Pattern recognition, face recognition, bioinformatics.\\n4. Independent Component Analysis (ICA)\\nDescription: ICA separates a multivariate signal into additive, independent components.\\nKey Points:\\nAssumes statistical independence of components.\\nUseful for blind source separation.\\nSensitive to noise.\\nApplications: Signal processing, brain imaging, finance.\\n5. UMAP (Uniform Manifold Approximation and Projection)\\nDescription: UMAP reduces dimensions while preserving the global structure of the data, using a manifold learning technique.\\nKey Points:\\nPreserves both local and global structure.\\nComputationally efficient.\\nRequires tuning of parameters.\\nApplications: Data visualization, clustering, pattern recognition.\\nC. Association\\n1. Apriori Algorithm\\nDescription: The Apriori algorithm identifies frequent itemsets in transactional data and generates association rules.\\nKey Points:\\nSimple and easy to implement.\\nCan handle large datasets.\\nComputationally expensive for large itemsets.\\nApplications: Market basket analysis, cross-selling strategies, web usage mining.\\n2. Eclat Algorithm\\nDescription: The Eclat algorithm uses depth-first search to find frequent itemsets, improving efficiency by reducing the number of database scans.\\nKey Points:\\nMore efficient than Apriori for large datasets.\\nUses vertical data format.\\nRequires sufficient memory for large itemsets.\\nApplications: Market basket analysis, bioinformatics, text mining.\\n3. Reinforcement Learning\\nReinforcement learning involves training agents to make a sequence of decisions by rewarding them for good actions and penalizing them for bad ones.\\n\\nA. Model-Free Methods\\n1. Q-Learning\\nDescription: Q-Learning learns the value of actions in states to maximize cumulative reward, updating Q-values based on the Bellman equation.\\nKey Points:\\nOff-policy learning method.\\nCan handle problems with stochastic transitions.\\nConvergence can be slow.\\nApplications: Robotics, game playing, autonomous vehicles.\\n2. Deep Q-Network (DQN)\\nDescription: DQN uses deep learning to approximate Q-values, enabling reinforcement learning in high-dimensional state spaces.\\nKey Points:\\nCombines Q-Learning with deep neural networks.\\nHandles large state spaces.\\nRequires extensive training.\\nApplications: Video games, robotics, control systems.\\n3. SARSA (State-Action-Reward-State-Action)\\nDescription: SARSA learns the value of the policy being followed by updating Q-values based on the state-action pairs encountered.\\nKey Points:\\nOn-policy learning method.\\nTakes into account the policyâ€™s behavior.\\nSensitive to the choice of policy.\\nApplications: Path planning, robotics, autonomous navigation.\\n4. Policy Gradient Methods (e.g., REINFORCE)\\nDescription: Policy gradient methods directly learn the policy that maps states to actions by optimizing the expected reward.\\nKey Points:\\nSuitable for continuous action spaces.\\nCan handle complex policies.\\nProne to high variance in gradient estimates.\\nApplications: Robotics, control systems, game playing.\\nB. Model-Based Methods\\n1. Deep Deterministic Policy Gradient (DDPG)\\nDescription: DDPG uses actor-critic methods for continuous action spaces, combining deep learning with deterministic policy gradients.\\nKey Points:\\nHandles continuous action spaces.\\nStable learning process.\\nRequires tuning of hyperparameters.\\nApplications: Robotics, autonomous driving, financial trading.\\n2. Proximal Policy Optimization (PPO)\\nDescription: PPO balances exploration and exploitation using a clipped objective function, improving policy stability and performance.\\nKey Points:\\nStable and efficient.\\nSuitable for complex tasks.\\nRequires careful tuning of clipping parameter.\\nApplications: Robotics, game playing, simulation-based optimization.\\n3. Trust Region Policy Optimization (TRPO)\\nDescription: TRPO ensures stable policy updates by optimizing within a trust region, preventing large updates that could degrade performance.\\nKey Points:\\nStable policy updates.\\nSuitable for high-dimensional problems.\\nComputationally intensive.\\nApplications: Robotics, game playing, resource management.\\nC. Value-Based Methods\\n1. Monte Carlo Methods\\nDescription: Monte Carlo methods estimate value functions based on averaging sample returns from multiple episodes.\\nKey Points:\\nSimple and easy to implement.\\nRequires complete episodes for updating.\\nHigh variance in estimates.\\nApplications: Game playing, inventory management, financial modeling.\\n2. Temporal Difference (TD) Learning\\nDescription: TD Learning combines Monte Carlo and dynamic programming ideas, updating value estimates based on bootstrapped predictions.\\nKey Points:\\nDoes not require complete episodes.\\nBalances bias and variance.\\nConvergence can be slow.\\nApplications: Game playing, robotics, control systems.\\n4. Ensemble Learning\\nEnsemble learning combines multiple models to improve performance by leveraging the strengths of each model.\\n\\n1. Bagging (e.g., Random Forest)\\nDescription: Bagging reduces variance by averaging predictions from multiple models trained on different subsets of data, typically using decision trees.\\nKey Points:\\nReduces overfitting.\\nImproves stability and accuracy.\\nRequires more computational resources.\\nApplications: Financial forecasting, image classification, healthcare diagnostics.\\n2. Boosting (e.g., AdaBoost, Gradient Boosting)\\nDescription: Boosting sequentially builds models to correct errors made by previous models, optimizing for accuracy.\\nKey Points:\\nHighly accurate and efficient.\\nCan handle different types of data.\\nProne to overfitting if not properly tuned.\\nApplications: Web search ranking, customer churn prediction, insurance risk prediction.\\n3. Stacking\\nDescription: Stacking combines multiple models by training a meta-model on their outputs to improve performance.\\nKey Points:\\nCan leverage different types of models.\\nCan improve generalization.\\nMore complex to implement and tune.\\nApplications: Predictive modeling, competition winning solutions, recommendation systems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure()\n",
        "llm=genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "5TNn9tJ_U7XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexing(documents,chunk_size=100,chunk_overlap=10):\n",
        "  documents=documents.split()\n",
        "  documents_chunk=[]\n",
        "  for i in  range(0,len(documents)+chunk_size,chunk_size):\n",
        "    if i==0:\n",
        "      documents_chunk.append(\" \".join(documents[i:i+chunk_size]))\n",
        "    else:\n",
        "      documents_chunk.append(\" \".join(documents[i-chunk_overlap:i+chunk_overlap]))\n",
        "    # documents_chunk.remove('')\n",
        "\n",
        "    vectorizer=TfidfVectorizer(max_features=110)\n",
        "    tfidf_matrix=vectorizer.fit_transform(documents_chunk)\n",
        "\n",
        "    return vectorizer,tfidf_matrix,documents_chunk"
      ],
      "metadata": {
        "id": "z3SwJpXeU7aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer,tfidf_matrix,documents_chunk=indexing(documents,chunk_size=100,chunk_overlap=10)"
      ],
      "metadata": {
        "id": "Nf4-eHfOgk63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieval(vectorizer,tfidf_matrix,input_text,documents_chunk,k):\n",
        "  input_vector=vectorizer.transform([input_text])\n",
        "  similarities=cosine_similarity(input_vector,tfidf_matrix).flatten()\n",
        "  most_similar_indexes=similarities.argsort()[::-1][:k]\n",
        "  relavant_chunk=[]\n",
        "\n",
        "  for idx in most_similar_indexes:\n",
        "    relavant_chunk.append(documents_chunk[k])\n",
        "    print(f'Document {idx}')\n",
        "    print(f'Text:{documents_chunk[idx]}')\n",
        "    print(f'Cosine similarity score:{similarities[idx]:.4f}\\n')\n",
        "\n",
        "  return relavant_chunk"
      ],
      "metadata": {
        "id": "l5_B6d5BiqbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generation(prompt,llm):\n",
        "  response=llm.generate_content(prompt)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "JIe0UKmUjHRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chain(input_text,vectorizer,tfidf_matrix,documents_chunk,llm,k):\n",
        "  relavant_chunk=retrieval(vectorizer,tfidf_matrix,input_text,documents_chunk,k)\n",
        "  prompt=f\"\"\"\n",
        "    You are a helpful Document Assistant that can answer questions based on the provided document.\n",
        "\n",
        "    Answer the following question: `{input_text}`\n",
        "    By searching the following document content: `{relavant_chunk}`\n",
        "\n",
        "    Only use factual information from the document content to answer the question.\n",
        "\n",
        "    If you feel like you don`t have enough information to answer the question, say directly \"I don`t know\".\n",
        "    Your answer should be detailed but easy to understand.\n",
        "    \"\"\"\n",
        "\n",
        "  response=generation(prompt,llm)\n",
        "  return response"
      ],
      "metadata": {
        "id": "WvfEbucsnjoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text=\"What is Machine Learning\"\n",
        "print(f\"Input: {input_text}\")\n",
        "response=rag_chain(input_text,vectorizer,tfidf_matrix,documents,llm,k=2)\n",
        "print(f'Output:{response}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "YkJrlK7snjsM",
        "outputId": "6ffed41f-42e7-49f3-fde3-e49ef540dc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: What is Machine Learning\n",
            "Document 0\n",
            "Text:M\n",
            "Cosine similarity score:0.5038\n",
            "\n",
            "Output:I don't know. The provided document does not contain the answer to this question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Aw_u5oZ8rzTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text=\"What is Supervised Learning\"\n",
        "print(f\"Input: {input_text}\")\n",
        "response=rag_chain(input_text,vectorizer,tfidf_matrix,documents,llm,k=2)\n",
        "print(f'Output:{response}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "CVZ1JSfRnj0T",
        "outputId": "174584f9-7f10-4c95-c58a-87ed090bd67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: What is Supervised Learning\n",
            "Document 0\n",
            "Text:M\n",
            "Cosine similarity score:0.3562\n",
            "\n",
            "Output:I don't know. This document does not have information about `Supervised Learning`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text=\"what are the content in the document\"\n",
        "print(f\"Input: {input_text}\")\n",
        "response=rag_chain(input_text,vectorizer,tfidf_matrix,documents,llm,k=2)\n",
        "print(f'Output:{response}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "eabClsPAsqUT",
        "outputId": "6bf575e7-2377-40b4-8a56-fb97d37fcdbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: what are the content in the document\n",
            "Document 0\n",
            "Text:M\n",
            "Cosine similarity score:0.2327\n",
            "\n",
            "Output:I don't know. The provided document content does not contain the string 'c'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLDnaQ18swfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}